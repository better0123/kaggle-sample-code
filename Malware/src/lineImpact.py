import scipy.stats
from collections import defaultdict
import numpy as np
import math
import sys

MIN_COUNT  = 2
MAX_RESULTS = 10000
file_dir   = '../data/lines.uniq'
labels     = [   1,    2,      3,    4,   5,    6,   7,    8,    9]
fileCounts = [1541, 2478,   2942,  475,  42,  751, 398, 1228, 1013]

EPS = float(sys.argv[1]) 
print "EPS factor:", EPS


opCounts = defaultdict(lambda: np.zeros(9+1, dtype=int)) 

for label in labels:
    f_name = file_dir + '/' + str(label) + '.txt'
    print "Reading:", f_name
    for f_line in open(f_name, 'r'):
        tokens = f_line.strip().split()
        count = int(tokens[0])
        if count < MIN_COUNT:
            break
        opLine = ' '.join(tokens[1:])
        opCounts[opLine][label] += count

print len(opCounts), "Unique strings found in all files"

def mask(posn, complement=False):
    a = np.zeros(9+1, dtype=int)
    a[posn] = 1
    if complement:
        return 1-a
    else:
        return a

print "Calculating impacts"
for label in labels:

    labelMask    = mask(posn=label, complement=False)
    #notLabelMask = mask(posn=label, complement=True )

    results = []
    for n, opLine in enumerate(opCounts):
        if n % 10000 == 0: print n, 

        hitsPerLabel = opCounts[opLine]

        #pk = hitsPerLabel[1:] *    labelMask[1:] + 1
        #qk = hitsPerLabel[1:] * notLabelMask[1:] + 1 

        pk = hitsPerLabel[1:]  # NOTE: not normalized 
        #pk = hitsPerLabel[1:]  * 100. / np.array(fileCounts)  # note: normalized
        qk = labelMask[1:] + EPS
        hitsThisLabel = hitsPerLabel[label] + 1
        #impact = -math.log(scipy.stats.entropy(pk=pk, qk=qk) / sum(pk))
        entropy = scipy.stats.entropy(pk=pk, qk=qk) 
        impact =  -math.log(entropy      / hitsThisLabel)
        """
        maxHitsThisLabel = fileCounts[label-1]
        pk_flat = pk*0 + 1. # flat distribution, no distinction between classes 
        entropy_flat = scipy.stats.entropy(pk=pk_flat, qk=qk) 
        impact -= -math.log(entropy_flat / maxHitsThisLabel)
        """
        maxHitsThisLabel = fileCounts[label-1]
        pk_prior = np.array(fileCounts)
        entropy_prior = scipy.stats.entropy(pk=pk_prior, qk=qk) 
        impact -= -math.log(entropy_prior / maxHitsThisLabel)

        results.append( (impact, tuple(hitsPerLabel),  opLine) ) 

        """
        pcts = 100. * hitsPerLabel[1:] / np.array(fileCounts)
        count_label = pcts[label-1]
        count_notLabel = sum(pcts) - count_label
        impact =  count_label * math.log((count_label + 1.) / (count_notLabel + 1.))
        results.append( (impact, tuple(pcts),  opLine) ) 
        """

    print "\nSorting results"
    results.sort(reverse=True)
    print "\nRESULTS for Label:", label
    for n, result in enumerate(results):
        impact, hitsPerLabel, opLine = result
        pcts = 100. * np.array(hitsPerLabel[1:]) / np.array(fileCounts)
        print "{0:1d} |{1:6d} |{2:7.2f} |".format(label, n, impact),
        for pct in pcts:
            print "{0:6.1f}".format(pct), 
        print " |", opLine
        print "                   |", 
        for hits in hitsPerLabel[1:]:
            print "{0:6d}".format(hits), 
        labelHitsPct = 100.* hitsPerLabel[label] / sum(hitsPerLabel[1:])
        print " | {0:7.2f}".format(labelHitsPct)
        print 

        if n > MAX_RESULTS: break


